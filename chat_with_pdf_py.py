# -*- coding: utf-8 -*-
"""pdf_chat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ume0P-IGDj68WUClr-iIGRIsIo08jFFb

#Importing the required library
"""

!pip install langchain
!pip install langchain_community
!pip install replicate
!pip install pypdf
!pip install chromadb
pip install sentence-transformers

"""Importing the required libraries and setting the replicate API"""

import os
import sys
from langchain.llms import Replicate
from langchain.vectorstores import Pinecone
from langchain.text_splitter import CharacterTextSplitter
from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings
from langchain.vectorstores import Chroma

# Replicate API token
os.environ['REPLICATE_API_TOKEN'] = "Your replicate api"

"""Creating a function to read the pdf and extract the text from it."""

from pypdf import PdfReader
def get_pdf_text(pdf):
    text = ""
    pdf_reader = PdfReader(pdf)
    for page in pdf_reader.pages:
      text += page.extract_text()
    return text

"""Storing the textual content of the input pdf in t"""

t = get_pdf_text('./heart.pdf')

print(t)

"""This code snippet splits a large text input into smaller, manageable chunks using a CharacterTextSplitter class. The splitting criteria are based on a separator (e.g., newline character) and chunk size/overlap settings. Then, it  reassembles the chunks back into documents or generates new documents based on the splitting strategy"""

#loader = PyPDFLoader('./heart.pdf')
#documents = loader.load()
documents = get_pdf_text('./heart.pdf')
# Split the documents into smaller chunks for processing
text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
chunks = text_splitter.split_text(documents)
texts= text_splitter.create_documents(chunks)
#texts = text_splitter.split_documents(documents)

"""Using HuggingFace Embeddings on text to and then store it in chroma vectorstore"""

#embeddings = HuggingFaceInstructEmbeddings(model_name="hkunlp/instructor-xl")
embeddings = HuggingFaceEmbeddings()
vectorstore = Chroma.from_documents(texts, embeddings,ids=None, collection_name="heart",persist_directory='./chroma_db')
vectorstore.persist()

"""Importing LLM Llama 3 from replicate tuning some of its parameters like Temperature which is used for randomness of the response by llm.
Then the system prompt that helps the llm understand its role pretty well.
Then the max tokens for restricting the size, there are more parameters to tune but these are basic.
"""

llm = Replicate(
    model="meta/meta-llama-3-8b",
    input={"temperature":0.6,
    "prompt" : "You are an helpful AI.You do not generate further questions and helpful answer of them",
    "max_tokens":2000,
    }
)

"""#ConversationalRetrievalChain
 It is a kind of chain used to be provided with a query and to answer it using documents retrieved from the query. It is one of the many possibilities to perform Retrieval-Augmented Generation.

But it wonâ€™t only answer your last query, it will also use the chat history to improve the quality of the RAG by taking into account past querie
"""

qa_chain = ConversationalRetrievalChain.from_llm(
    llm,
    vectorstore.as_retriever(search_kwargs={'k': 2}), # Making the vector store retrievable so that the data of pdf can be used to answer the prompts . k means the retrieval system will find and return the two most relevant documents (based on some similarity metric) to the question posed by the LLM.
    return_source_documents=True # returns the source doucment also
)

chat_history = []  #creating the chat history
while True:
    query = input('Prompt: ')  # taking the prompt as input
    if query.lower() in ["exit", "quit", "q"]:  # exit statements
        print('Exiting')
        sys.exit()
    result = qa_chain({'question': query, 'chat_history': chat_history})  # retrival chain to answer our prompt
    print('Answer: ' + result['answer'] + '\n')
    chat_history.append((query, result['answer']))  # appending the chat history

